{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bsayZPkCiBJv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lu1dpltohpb0"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашняя + практическая работа 2"
      ],
      "metadata": {
        "id": "lTeEBs2bh6iR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание. \n",
        "\n",
        "Реализуйте backward для Polynomial 0.5 * (5 * input ** 3 - 3 * input)"
      ],
      "metadata": {
        "id": "bsayZPkCiBJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from torch.autograd import Function"
      ],
      "metadata": {
        "id": "xgrH3vvnjanM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_tensor_params(*tensors):\n",
        "  for x in tensors:\n",
        "    print('---')\n",
        "    print(f\"data - {x.data}\")\n",
        "    print(f\"grad - {x.grad}\")\n",
        "    print(f\"grad_fn - {x.grad_fn}\")\n",
        "    print(f\"req_grad - {x.requires_grad}\")\n",
        "    print(f\"is_leaf - {x.is_leaf}\")"
      ],
      "metadata": {
        "id": "ASpORIs5jQO8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class Polynomial(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we receive a Tensor containing the input and return\n",
        "        a Tensor containing the output. ctx is a context object that can be used\n",
        "        to stash information for backward computation. You can cache arbitrary\n",
        "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        return (7.5 * input ** 2 - 1.5) * grad_output"
      ],
      "metadata": {
        "id": "MW21DYh_iAdz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor(1., requires_grad=True)\n",
        "output = Polynomial.apply(input)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKtCADwti0p8",
        "outputId": "96c4a6f6-2e90-42e2-f85f-d99f5003f54f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., grad_fn=<PolynomialBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs = np.arange(-5, 5, 0.25)\n",
        "ys = Polynomial.apply(xs)\n",
        "plt.plot(xs, ys)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "D9k8PGmei66N",
        "outputId": "d1424702-f9d8-407d-880a-61284472bbec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcace9f9730>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.backward()\n",
        "show_tensor_params(output)\n",
        "x = show_tensor_params(input)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gps2Rfypi8Xl",
        "outputId": "26da3147-8ead-4721-9bb7-dd8dd3156fa0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "data - 1.0\n",
            "grad - None\n",
            "grad_fn - <torch.autograd.function.PolynomialBackward object at 0x7fcacf3876d0>\n",
            "req_grad - True\n",
            "is_leaf - False\n",
            "---\n",
            "data - 1.0\n",
            "grad - 6.0\n",
            "grad_fn - None\n",
            "req_grad - True\n",
            "is_leaf - True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-168b1ceafb31>:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
            "  print(f\"grad - {x.grad}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Создание собственной библиотеки автоматического дифференцирования\n",
        "\n",
        "Написать собственный движок автоматического дифференцирования."
      ],
      "metadata": {
        "id": "GAzquTeYjpog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Value:\n",
        "\t\"\"\" stores a single scalar value and its gradient \"\"\"\n",
        "\n",
        "\tdef __init__(self, data, _children=(), _op=''):\n",
        "\t\tself.data = data\n",
        "\t\tself.grad = 0\n",
        "\t\t# internal variables used for autograd graph construction\n",
        "\t\tself._backward = lambda: None  # function\n",
        "\t\tself._prev = set(_children)  # set of Value objects\n",
        "\t\tself._op = _op  # the op that produced this node, string ('+', '-', ....)\n",
        "\n",
        "\tdef __add__(self, other):\n",
        "\t\tother = other if isinstance(other, Value) else Value(other)\n",
        "\t\tout = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "\t\tdef _backward():\n",
        "\t\t\tself.grad += out.grad\n",
        "\t\t\tother.grad += out.grad\n",
        "\n",
        "\t\tout._backward = _backward\n",
        "\n",
        "\t\treturn out\n",
        "\n",
        "\tdef __mul__(self, other):\n",
        "\t\tother = other if isinstance(other, Value) else Value(other)\n",
        "\t\tout = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "\t\tdef _backward():\n",
        "\t\t\tself.grad += other.data * out.grad\n",
        "\t\t\tother.grad += self.data * out.grad\n",
        "\n",
        "\t\tout._backward = _backward\n",
        "\n",
        "\t\treturn out\n",
        "\n",
        "\tdef __pow__(self, other):\n",
        "\t\tassert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "\t\tout = Value(self.data ** other, (self,), f'**{other}')\n",
        "\n",
        "\t\tdef _backward():\n",
        "\t\t\tself.grad += other * self.data ** (other - 1) * out.grad\n",
        "\n",
        "\t\tout._backward = _backward\n",
        "\n",
        "\t\treturn out\n",
        "\n",
        "\tdef relu(self):\n",
        "\t\tout = Value(self.data if self.data > 0 else 0, (self,), 'relu')\n",
        "\n",
        "\t\tdef _backward():\n",
        "\t\t\tself.grad += out.grad if self.data > 0 else 0\n",
        "\n",
        "\t\tout._backward = _backward\n",
        "\n",
        "\t\treturn out\n",
        "\n",
        "\tdef backward(self):\n",
        "\n",
        "\t\t# topological order all of the children in the graph\n",
        "\t\ttopo = []\n",
        "\t\tvisited = set()\n",
        "\n",
        "\t\tdef build_topo(v):\n",
        "\t\t\tif v not in visited:\n",
        "\t\t\t\tvisited.add(v)\n",
        "\t\t\t\tfor child in v._prev:\n",
        "\t\t\t\t\tbuild_topo(child)\n",
        "\t\t\t\ttopo.append(v)\n",
        "\n",
        "\t\tbuild_topo(self)\n",
        "\n",
        "\t\t# go one variable at a time and apply the chain rule to get its gradient\n",
        "\t\tself.grad = 1\n",
        "\t\tfor v in reversed(topo):\n",
        "\t\t\tv._backward()\n",
        "\n",
        "\tdef __neg__(self):  # -self\n",
        "\t\treturn self * -1\n",
        "\n",
        "\tdef __radd__(self, other):  # other + self\n",
        "\t\treturn self + other\n",
        "\n",
        "\tdef __sub__(self, other):  # self - other\n",
        "\t\treturn self + (-other)\n",
        "\n",
        "\tdef __rsub__(self, other):  # other - self\n",
        "\t\treturn other + (-self)\n",
        "\n",
        "\tdef __rmul__(self, other):  # other * self\n",
        "\t\treturn self * other\n",
        "\n",
        "\tdef __truediv__(self, other):  # self / other\n",
        "\t\treturn self * other ** -1\n",
        "\n",
        "\tdef __rtruediv__(self, other):  # other / self\n",
        "\t\treturn other * self ** -1\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\treturn f\"Value(data={self.data}, grad={self.grad})\"\n"
      ],
      "metadata": {
        "id": "eLh9iFLnj37E"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_sanity_check():\n",
        "\n",
        "    x = Value(-4.0)\n",
        "    z = 2 * x + 2 + x\n",
        "  \n",
        "    q = z.relu() + z * x\n",
        "    h = (z * z).relu()\n",
        "    y = h + q + q * x\n",
        "    y.backward()\n",
        "    xmg, ymg = x, y\n",
        "\n",
        "    x = torch.Tensor([-4.0]).double()\n",
        "    x.requires_grad = True\n",
        "    z = 2 * x + 2 + x\n",
        "    q = z.relu() + z * x\n",
        "    h = (z * z).relu()\n",
        "    y = h + q + q * x\n",
        "    y.backward()\n",
        "    xpt, ypt = x, y\n",
        "\n",
        "    \n",
        "    # forward pass went well\n",
        "    assert ymg.data == ypt.data.item()\n",
        "    # backward pass went well\n",
        "    # print(xmg, xpt, xpt.grad)\n",
        "    assert xmg.grad == xpt.grad.item()\n",
        "    print(\"test_sanity_check passed\")\n",
        "\n",
        "def test_more_ops():\n",
        "\n",
        "    a = Value(-4.0)\n",
        "    b = Value(2.0)\n",
        "    c = a + b\n",
        "    d = a * b + b**3\n",
        "    c += c + 1\n",
        "    c += 1 + c + (-a)\n",
        "    d += d * 2 + (b + a).relu()\n",
        "    d += 3 * d + (b - a).relu()\n",
        "    e = c - d\n",
        "    f = e**2\n",
        "    g = f / 2.0\n",
        "    g += 10.0 / f\n",
        "    g.backward()\n",
        "    amg, bmg, gmg = a, b, g\n",
        "\n",
        "    a = torch.Tensor([-4.0]).double()\n",
        "    b = torch.Tensor([2.0]).double()\n",
        "    a.requires_grad = True\n",
        "    b.requires_grad = True\n",
        "    c = a + b\n",
        "    d = a * b + b**3\n",
        "    c = c + c + 1\n",
        "    c = c + 1 + c + (-a)\n",
        "    d = d + d * 2 + (b + a).relu()\n",
        "    d = d + 3 * d + (b - a).relu()\n",
        "    e = c - d\n",
        "    f = e**2\n",
        "    g = f / 2.0\n",
        "    g = g + 10.0 / f\n",
        "    g.backward()\n",
        "    apt, bpt, gpt = a, b, g\n",
        "\n",
        "    tol = 1e-6\n",
        "    # forward pass went well\n",
        "    assert abs(gmg.data - gpt.data.item()) < tol\n",
        "    # backward pass went well\n",
        "    assert abs(amg.grad - apt.grad.item()) < tol\n",
        "    assert abs(bmg.grad - bpt.grad.item()) < tol\n",
        "    print(\"test_more_ops passed\")"
      ],
      "metadata": {
        "id": "O7KEFnIbkCqM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Value(-4.0)\n",
        "b = Value(2.0)\n",
        "d = Value(3.0)"
      ],
      "metadata": {
        "id": "Wejdng6gkDgI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = a + b\n",
        "e = c * d\n",
        "e.backward()"
      ],
      "metadata": {
        "id": "TnCW3WdnkG5e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sanity_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPuOdAKwkJC4",
        "outputId": "9f93cb9e-22df-4bf7-9eb9-d869f807473d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_sanity_check passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_more_ops()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t05eMSjJkLt5",
        "outputId": "44d2c84c-f2df-490a-80fb-35f1f48ed9c0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_more_ops passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение на основе собственной бибилотеки"
      ],
      "metadata": {
        "id": "nJ5OMkSunfU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Многослойный перцептрон на основе класса Value"
      ],
      "metadata": {
        "id": "buJ78OXEnmQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "class Module:\n",
        "\n",
        "\tdef zero_grad(self):\n",
        "\t\tfor p in self.parameters():\n",
        "\t\t\tp.grad = 0\n",
        "\n",
        "\tdef parameters(self):\n",
        "\t\treturn []\n",
        "\n",
        "\n",
        "class Neuron(Module):\n",
        "\n",
        "\tdef __init__(self, nin, nonlin=True):\n",
        "\t\tself.w = [Value(random.random()) for _ in range(nin)]\n",
        "\t\tself.b = Value(random.random())\n",
        "\t\tself.nonlin = nonlin\n",
        "\n",
        "\tdef __call__(self, x):\n",
        "\t\tact = sum(w * xi for w, xi in zip(self.w, x)) + self.b\n",
        "\t\treturn act.relu() if self.nonlin else act\n",
        "\n",
        "\tdef parameters(self):\n",
        "\t\treturn self.w + [self.b]\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\treturn f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
        "\n",
        "\n",
        "class Layer(Module):\n",
        "\n",
        "\tdef __init__(self, nin, nout, **kwargs):\n",
        "\t\tself.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
        "\n",
        "\tdef __call__(self, x):\n",
        "\t\tout = [n(x) for n in self.neurons]\n",
        "\t\treturn out[0] if len(out) == 1 else out\n",
        "\n",
        "\tdef parameters(self):\n",
        "\t\treturn [p for n in self.neurons for p in n.parameters()]\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\treturn f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
        "\n",
        "\n",
        "class MLP(Module):\n",
        "\n",
        "\tdef __init__(self, nin, nouts):\n",
        "\t\tsz = [nin] + nouts\n",
        "\t\tself.layers = [Layer(sz[i], sz[i + 1], nonlin=(i != len(nouts) - 1)) for i in range(len(nouts))]\n",
        "\n",
        "\tdef __call__(self, x):\n",
        "\t\tfor layer in self.layers:\n",
        "\t\t\tx = layer(x)\n",
        "\t\treturn x\n",
        "\n",
        "\tdef parameters(self):\n",
        "\t\treturn [p for layer in self.layers for p in layer.parameters()]\n",
        "\n",
        "\tdef __repr__(self):\n",
        "\t\trepr = '\\n'.join(str(layer) for layer in self.layers)\n",
        "\t\treturn f\"MLP of [{repr}]\"\n"
      ],
      "metadata": {
        "id": "_7u-AVmGnpE8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение многослойного перцептрона"
      ],
      "metadata": {
        "id": "iUQ2WUsInxaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP(3, [4, 4, 1])\n",
        "print(model)\n",
        "print(\"number of parameters\", len(model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxtoi5Rrn8OK",
        "outputId": "5122403f-66ae-4b78-fea6-b31d7612ffa5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP of [Layer of [ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3)]\n",
            "Layer of [ReLUNeuron(4), ReLUNeuron(4), ReLUNeuron(4), ReLUNeuron(4)]\n",
            "Layer of [LinearNeuron(4)]]\n",
            "number of parameters 41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs = [\n",
        "  [2.0, 3.0, -1.0],\n",
        "  [3.0, -1.0, 0.5],\n",
        "  [0.5, 1.0, 1.0],\n",
        "  [1.0, 1.0, -1.0],\n",
        "]\n",
        "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
      ],
      "metadata": {
        "id": "a-ZJmM3soTTy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(50):\n",
        "\n",
        "\t# forward\n",
        "\tloss = 0\n",
        "\tfor x, y in zip(xs, ys):\n",
        "\t\ty_hat = model(x)\n",
        "\t\tloss += (y_hat - y) ** 2\n",
        "\n",
        "\t# backward\n",
        "\tmodel.zero_grad()\n",
        "\tloss.backward()\n",
        "\n",
        "\tfor p in model.parameters():\n",
        "\t\tp.data -= 0.01 * p.grad\n",
        "\tprint(f\"loss = {loss.data:.3f}\")\n"
      ],
      "metadata": {
        "id": "fCLq8JjAoY4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6abbb099-8f7f-42e5-e7b0-f1de06d72fdb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss = 591.087\n",
            "loss = 6.271\n",
            "loss = 4.288\n",
            "loss = 4.244\n",
            "loss = 4.206\n",
            "loss = 4.175\n",
            "loss = 4.148\n",
            "loss = 4.125\n",
            "loss = 4.106\n",
            "loss = 4.090\n",
            "loss = 4.076\n",
            "loss = 4.064\n",
            "loss = 4.054\n",
            "loss = 4.046\n",
            "loss = 4.039\n",
            "loss = 4.033\n",
            "loss = 4.028\n",
            "loss = 4.024\n",
            "loss = 4.020\n",
            "loss = 4.017\n",
            "loss = 4.014\n",
            "loss = 4.012\n",
            "loss = 4.010\n",
            "loss = 4.009\n",
            "loss = 4.007\n",
            "loss = 4.006\n",
            "loss = 4.005\n",
            "loss = 4.004\n",
            "loss = 4.004\n",
            "loss = 4.003\n",
            "loss = 4.003\n",
            "loss = 4.002\n",
            "loss = 4.002\n",
            "loss = 4.002\n",
            "loss = 4.001\n",
            "loss = 4.001\n",
            "loss = 4.001\n",
            "loss = 4.001\n",
            "loss = 4.001\n",
            "loss = 4.001\n",
            "loss = 4.001\n",
            "loss = 4.000\n",
            "loss = 4.000\n",
            "loss = 4.000\n",
            "loss = 4.000\n",
            "loss = 4.000\n",
            "loss = 4.000\n",
            "loss = 4.000\n",
            "loss = 4.000\n",
            "loss = 4.000\n"
          ]
        }
      ]
    }
  ]
}